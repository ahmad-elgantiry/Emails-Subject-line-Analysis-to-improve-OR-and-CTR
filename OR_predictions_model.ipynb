{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model To predict Emails Open Rates (OR%) , Based on the following Features \n",
    "- Time ( month of the year , quarter of the year , day of the week )\n",
    "- Pirority of email , main or reminder \n",
    "- Category ( interval , final )\n",
    "- Number of emails sent in each subject\n",
    "- Constructed Features\n",
    " - Length of the subject \n",
    " - Personalization of Email\n",
    " - Having emoji or not\n",
    " - Casual tone or formal \n",
    " - Urgency at the subject \n",
    " - Metion of price or discount in the subject \n",
    " - Imperative tone in the subject \n",
    " - Mention of product in the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r reading the data as a dataframe\n",
    "df = pd.read_csv(\"model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [ \"translation\", \"CTR clean %\",\"Date\",\"Month\"]  \n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df['Sent date'] = pd.to_datetime(df['Sent date'], errors='coerce')\n",
    "\n",
    "# Create a Month column (numeric)\n",
    "df['Month'] = df['Sent date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def count_word_letters(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1️⃣ Remove emoji codes like &#x1f4b0; or &#128512;\n",
    "    text = re.sub(r'&#x?[0-9A-Fa-f]+;?', '', text)\n",
    "    \n",
    "    # 2️⃣ Remove template/formatted patterns {% ... %} and {{ ... }}\n",
    "    text = re.sub(r'{%.*?%}', '', text)\n",
    "    text = re.sub(r'{{.*?}}', '', text)\n",
    "    \n",
    "    # 3️⃣ Remove HTML entities like &amp;, &lt;, etc.\n",
    "    text = re.sub(r'&[a-zA-Z]+;', '', text)\n",
    "    \n",
    "    # 4️⃣ Count only alphabetic letters (a-z, A-Z)\n",
    "    return len(re.findall(r'[A-Za-z]', text))\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['letter_count'] = df['Subject'].apply(count_word_letters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i will drop subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='Subject', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next , I will confirm that the values in \"OR clean %\" is a valid percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values are between 0 and 1\n",
    "valid_percentage = df['OR clean %'].between(0, 1).all()\n",
    "\n",
    "if valid_percentage:\n",
    "    print(\"All values in 'OR clean %' are valid percentages (0 to 1).\")\n",
    "else:\n",
    "    # Show which rows are invalid\n",
    "    invalid_rows = df[~df['OR clean %'].between(0, 1)]\n",
    "    print(\"Some values are outside 0 to 1:\")\n",
    "    display(invalid_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_column_outliers(df, columns=['Sendings', 'OR clean %'], threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers in each specified numeric column using Z-score.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to check\n",
    "    - threshold: Z-score cutoff (default 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary: column name -> DataFrame of outlier rows\n",
    "    \"\"\"\n",
    "    outlier_dict = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col]\n",
    "            z_scores = np.abs(stats.zscore(col_data.dropna()))\n",
    "            col_outliers = df.loc[col_data.dropna().index[z_scores > threshold]]\n",
    "            outlier_dict[col] = col_outliers\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "            outlier_dict[col] = pd.DataFrame()  # empty DataFrame if column not found\n",
    "    \n",
    "    return outlier_dict\n",
    "\n",
    "# Example usage\n",
    "outliers = get_column_outliers(df, columns=['Sendings', 'OR clean %'], threshold=3)\n",
    "\n",
    "print(\"Outliers in 'Sendings':\")\n",
    "display(outliers['Sendings'])\n",
    "\n",
    "print(\"Outliers in 'OR clean %':\")\n",
    "display(outliers['OR clean %'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example: ensure units are consistent first\n",
    "# For demonstration, assume Sendings are already in correct units\n",
    "# If not, apply a conversion here, e.g.,\n",
    "# df['Sendings'] = df['Sendings'] * 1000  # convert thousands to units\n",
    "\n",
    "# Set Winsorization thresholds (percentiles)\n",
    "lower_pct = 0.01  # 1st percentile\n",
    "upper_pct = 0.99  # 99th percentile\n",
    "\n",
    "# List of columns to Winsorize\n",
    "columns_to_winsorize = ['Sendings', 'OR clean %']\n",
    "\n",
    "for col in columns_to_winsorize:\n",
    "    lower = df[col].quantile(lower_pct)\n",
    "    upper = df[col].quantile(upper_pct)\n",
    "    df[col] = np.clip(df[col], lower, upper)  # Cap values at thresholds\n",
    "    print(f\"{col}: capped below {lower:.3f}, capped above {upper:.3f}\")\n",
    "\n",
    "# Now df has Winsorized columns ready for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confirming that Data is valid for regression models \n",
    " - Cheking correlation \n",
    " - Ckeking multicoliniarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check correlation \n",
    "target = \"OR clean %\"  \n",
    "\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "corr_with_target = numeric_df.corr()[target].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n✅ Correlation with target:\")\n",
    "print(corr_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check multicolinearity \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "X = numeric_df.drop(columns=[target])  \n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(\"\\n✅ VIF to detect multicollinearity:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous result shows that there is no multicolinearity effect \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building bionomial GLM  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only valid OR clean % values\n",
    "# df = df[(df['OR clean %'] >= 0) & (df['OR clean %'] <= 1)].copy()\n",
    "\n",
    "# Convert 'Sent date' to datetime\n",
    "# df['Sent date'] = pd.to_datetime(df['Sent date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opens'] = (df['OR clean %'] * df['Sendings']).round().astype(int)\n",
    "df['fails'] = (df['Sendings'] - df['opens']).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['DD', 'Category name', 'Main vs REM', 'Month']\n",
    "\n",
    "# Create dummies\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Select predictor columns (exclude target and weights)\n",
    "predictor_cols = [c for c in df.columns if c not in ['OR clean %', 'Sendings', 'opens', 'fails']]\n",
    "\n",
    "# Ensure numeric float\n",
    "df[predictor_cols] = df[predictor_cols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictor_cols].astype(float)\n",
    "y = df[['opens','fails']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test, has_constant='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is the preprocessed DataFrame used in GLM training\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Get all categorical columns (dummy columns)\n",
    "dummy_cols = [col for col in X_train.columns if '_' in col]\n",
    "\n",
    "# 2️⃣ Map each original categorical column to its categories\n",
    "categorical_columns = {}\n",
    "for col in dummy_cols:\n",
    "    # The dummy column name format is usually: 'OriginalColumn_Category'\n",
    "    original_col, category = col.split('_', 1)\n",
    "    if original_col not in categorical_columns:\n",
    "        categorical_columns[original_col] = []\n",
    "    categorical_columns[original_col].append(category)\n",
    "\n",
    "print(\"Categorical columns and their categories:\")\n",
    "for col, cats in categorical_columns.items():\n",
    "    print(f\"{col}: {cats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_binom = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "model_result = glm_binom.fit()\n",
    "print(model_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_or = model_result.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rate = y_test[:,0] / (y_test[:,0] + y_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test_rate, y_pred_or)\n",
    "print(f\"MAE on test set: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_test_eval = X_test.copy()\n",
    "df_test_eval['y_pred'] = y_pred_or\n",
    "df_test_eval['y_true_rate'] = y_test_rate\n",
    "\n",
    "top_10pct = df_test_eval.sort_values('y_pred', ascending=False).head(int(len(df_test_eval)*0.1))\n",
    "top_decile_mean = top_10pct['y_true_rate'].mean()\n",
    "overall_mean = y_test_rate.mean()\n",
    "lift = top_decile_mean / overall_mean\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Mean predicted vs mean actual\n",
    "# -------------------------------\n",
    "mean_pred = np.mean(y_pred_or)\n",
    "mean_actual = np.mean(y_test_rate)\n",
    "\n",
    "print(f\"Top-decile lift: {lift:.2f}\")\n",
    "print(f\"Mean predicted OR clean %: {mean_pred:.4f}\")\n",
    "print(f\"Mean actual OR clean %:    {mean_actual:.4f}\")\n",
    "# -------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. Prepare Data\n",
    "# ================================\n",
    "\n",
    "\n",
    "# Compute successes/failures for Binomial GLM as integers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. Preprocessing: categorical → dummies\n",
    "# ================================\n",
    "\n",
    "# ================================\n",
    "# 4. Train/Test Split (random)\n",
    "# ================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add intercept\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. Fit Binomial GLM\n",
    "# ================================\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6. Predictions on test set\n",
    "# ================================\n",
    "\n",
    "\n",
    "# True rate for evaluation\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 7. Evaluation Metrics\n",
    "# ================================\n",
    "# MAE\n",
    "\n",
    "\n",
    "# Calibration curve\n",
    "plt.scatter(y_pred_or, y_test_rate, alpha=0.5)\n",
    "plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"Predicted OR clean %\")\n",
    "plt.ylabel(\"Actual OR clean %\")\n",
    "plt.title(\"Calibration Scatter\")\n",
    "plt.show()\n",
    "\n",
    "# Top-decile lift\n",
    "df_test_eval = X_test.copy()\n",
    "df_test_eval['y_pred'] = y_pred_or\n",
    "df_test_eval['y_true_rate'] = y_test_rate\n",
    "\n",
    "top_10pct = df_test_eval.sort_values('y_pred', ascending=False).head(int(len(df_test_eval)*0.1))\n",
    "top_decile_mean = top_10pct['y_true_rate'].mean()\n",
    "overall_mean = y_test_rate.mean()\n",
    "lift = top_decile_mean / overall_mean\n",
    "print(f\"Top-decile lift: {lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_or_and_opens(model, emails_list, predictor_cols, categorical_cols=['DD', 'Category name', 'Main vs REM', 'Month']):\n",
    "    \"\"\"\n",
    "    Predict OR clean % and estimated opens/fails for one or multiple emails.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : statsmodels.GLMResults\n",
    "        Trained Binomial GLM model.\n",
    "    emails_list : list of dicts\n",
    "        Each dict contains features for one email. Must include 'Sendings'.\n",
    "    predictor_cols : list\n",
    "        Numeric + dummy predictor columns used in training (exclude 'Sendings').\n",
    "    categorical_cols : list\n",
    "        Categorical columns to convert to dummies.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    result_df : pandas.DataFrame\n",
    "        DataFrame containing predicted OR clean %, predicted opens, predicted fails.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert list of dicts to DataFrame\n",
    "    df_new = pd.DataFrame(emails_list)\n",
    "\n",
    "    if 'Sendings' not in df_new.columns:\n",
    "        raise ValueError(\"Each email must include 'Sendings' for trial counts.\")\n",
    "\n",
    "    # Convert 'Sent date' to numeric if present\n",
    "    if 'Sent date' in df_new.columns:\n",
    "        df_new['Sent_date_ordinal'] = pd.to_datetime(df_new['Sent date'], errors='coerce').map(lambda x: x.toordinal())\n",
    "\n",
    "    # Create dummies\n",
    "    df_new = pd.get_dummies(df_new, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Add missing columns as zeros\n",
    "    for col in predictor_cols:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 0\n",
    "\n",
    "    # Keep only predictor columns in the correct order\n",
    "    X_new = df_new[predictor_cols].astype(float)\n",
    "    X_new = sm.add_constant(X_new, has_constant='add')\n",
    "\n",
    "    # Predict OR clean %\n",
    "    y_pred_or = model.predict(X_new)\n",
    "\n",
    "    # Calculate predicted opens/fails\n",
    "    pred_opens = y_pred_or * df_new['Sendings']\n",
    "    pred_fails = df_new['Sendings'] - pred_opens\n",
    "\n",
    "    # Combine into result DataFrame\n",
    "    result_df = df_new.copy()\n",
    "    result_df['Predicted_OR'] = y_pred_or\n",
    "    result_df['Predicted_Opens'] = pred_opens\n",
    "    result_df['Predicted_Fails'] = pred_fails\n",
    "\n",
    "    return result_df[['Predicted_OR', 'Predicted_Opens', 'Predicted_Fails']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = [\n",
    "    {\n",
    "        'Sendings': 1500,\n",
    "        'length of subject': 45,\n",
    "        'Personalization': 1,\n",
    "        'Emoji': 0,\n",
    "        'Urgency': 1,\n",
    "        'Tone': 0,\n",
    "        'Price or Discount': 1,\n",
    "        'imperative ': 0,\n",
    "        'product': 1,\n",
    "        'Month': 'Month_11',\n",
    "        'DD': \"DD_Thursday\",\n",
    "        'Category name': 'Promo',\n",
    "        'Main vs REM': 'Main'\n",
    "    },\n",
    "    {\n",
    "        'Sendings': 800,\n",
    "        'length of subject': 30,\n",
    "        'Personalization': 0,\n",
    "        'Emoji': 1,\n",
    "        'Urgency': 0,\n",
    "        'Tone': 1,\n",
    "        'Price or Discount': 0,\n",
    "        'imperative ': 1,\n",
    "        'product': 0,\n",
    "        'Month': 'Month_02',\n",
    "        'DD': 'DD_Tuesday',\n",
    "        'Category name': 'Update',\n",
    "        'Main vs REM': 'REM'\n",
    "    }\n",
    "]\n",
    "\n",
    "results = predict_or_and_opens(model_result, emails, predictor_cols)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
